# Configuration for Transformer model
data:
  num_samples: 1000
  channels: 14
  seq_len: 128
  sampling_rate: 256
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  patient_level_split: true
  synthetic_data: true

model:
  name: "transformer"
  channels: 14
  seq_len: 128
  num_classes: 2
  dropout: 0.1
  
  transformer:
    d_model: 128
    nhead: 8
    num_layers: 4
    dim_feedforward: 512

training:
  batch_size: 16  # Smaller batch size for transformer
  num_epochs: 50
  learning_rate: 0.0001  # Lower learning rate for transformer
  weight_decay: 1e-4
  scheduler: "cosine"
  early_stopping_patience: 10
  gradient_clip_norm: 1.0

loss:
  name: "cross_entropy"
  class_weights: null

evaluation:
  metrics: ["auroc", "auprc", "sensitivity", "specificity", "ppv", "npv", "brier_score", "ece"]
  calibration_bins: 10
  threshold_tuning: true
  per_channel_analysis: true

device:
  auto_detect: true
  fallback_order: ["cuda", "mps", "cpu"]

logging:
  level: "INFO"
  log_dir: "logs"
  tensorboard: true
  wandb: false

seed: 42
deterministic: true
